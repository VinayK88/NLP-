{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, as long as we're only *getting* information from Reddit, and not posting information to it, the Reddit API doesn't require authentication. So we don't have to find and store any keys or tokens. Let's go ahead and import PRAW, the Python Reddit API Wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we don't have to authenticate, it is considered basic courtesy to at least identify yourself in some fashion when using Reddit's API. When we build our API object, we pass it a parameter called user_agent, which is simply a string identifying your bot (it will show up with this name in Reddit access logs). It's best practice to put your email address or your reddit username in the user_agent string so you can be contacted if necessary. \n",
    "\n",
    "We use a method called ``Reddit`` and store its output in a variable. That variable is our API access to Reddit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = praw.Reddit(user_agent=\"IU Social Media Mining by vmalic@indiana.edu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting a User and Their Submissions\n",
    "\n",
    "We get a reddit user by calling the method ``get_redditor`` on the API and supplying a user name. I'm going to use a username that you can find in the PRAW documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user = r.get_redditor(\"MattDamon_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each reddit user (or redditor) has a small amount of information associated with it. Reddit is more content-centered, in contrast to Facebook, which is more user-centered. Therefore, there actually isn't a lot of info directly associated with a given user. In fact, all you can really find out about a user is their name (which you already know, if you're using the function ``get_redditor``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MattDamon_\n"
     ]
    }
   ],
   "source": [
    "print(user.name) # The user name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, if you're mining Reddit, you'll be more interested in the content associated with the user: the submissions they've made, the comments they've made, the items they've upvoted or downvoted, etc. \n",
    "\n",
    "Once we have a user, we can use a set a functions to get *that set of content associated with the user*. \n",
    "\n",
    "For example, ``get_upvoted`` gets all the submissions this user has upvoted. ``get_submitted`` gets all the submissions this user has made. However! Note that when you call these methods, you get a strange object called a ``generator``. You *can't* inspect a generator directly. The *only* way to access items inside them is with a ``for`` loop.\n",
    "\n",
    "Generators may strike you as odd as first, but you'll run into generators frequently as they're often a more efficient way to code things. If a normal list has 1000 items in it, that's taking up space in memory. With a generator, you only have to summon one object at a time, which eases memory use. \n",
    "\n",
    "In most practical situations, you'll only need generators because for each item you encounter in the sequence, you can process it somehow and store only the information you need in another variable, like an empty list you've initialized.\n",
    "\n",
    "One problem you may have with generators is that if you're iterating through a generator of objects you may not be sure what methods or attributes are available to you. There are two ways to fix this situation. First, go to the API documentation: it'll tell you what's available. Second, you can call a generator with a small amount of items and apply the ``list`` function to it to force-convert the generator to a list. \n",
    "\n",
    "Note that all the ``get_`` methods in PRAW take an argument called ``limit``, which indicates how many results you want. The default value is 25. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "submissions = user.get_submitted(limit=5)\n",
    "\n",
    "print(type(submissions))\n",
    "\n",
    "#Convert the generator into a list\n",
    "\n",
    "submissions = list(submissions)\n",
    "print(type(submissions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_api_link', '_comment_sort', '_comments', '_comments_by_id', '_extract_more_comments', '_get_json_dict', '_has_fetched', '_info_url', '_insert_comment', '_methods', '_orphaned', '_params', '_populate', '_post_populate', '_replaced_more', '_underscore_names', '_uniq', '_update_comments', 'add_comment', 'approve', 'approved_by', 'archived', 'author', 'author_flair_css_class', 'author_flair_text', 'banned_by', 'clear_vote', 'clicked', 'comments', 'created', 'created_utc', 'delete', 'distinguish', 'distinguished', 'domain', 'downs', 'downvote', 'edit', 'edited', 'from_api_response', 'from_id', 'from_json', 'from_url', 'fullname', 'get_duplicates', 'get_flair_choices', 'gild', 'gilded', 'has_fetched', 'hidden', 'hide', 'hide_score', 'id', 'ignore_reports', 'is_self', 'json_dict', 'likes', 'link_flair_css_class', 'link_flair_text', 'lock', 'locked', 'mark_as_nsfw', 'media', 'media_embed', 'mod_reports', 'name', 'num_comments', 'num_reports', 'over_18', 'permalink', 'post_hint', 'preview', 'quarantine', 'reddit_session', 'refresh', 'removal_reason', 'remove', 'replace_more_comments', 'report', 'report_reasons', 'save', 'saved', 'score', 'secure_media', 'secure_media_embed', 'select_flair', 'selftext', 'selftext_html', 'set_contest_mode', 'set_flair', 'set_suggested_sort', 'short_link', 'stickied', 'sticky', 'subreddit', 'subreddit_id', 'suggested_sort', 'thumbnail', 'title', 'undistinguish', 'unhide', 'unignore_reports', 'unlock', 'unmark_as_nsfw', 'unsave', 'unset_contest_mode', 'unsticky', 'ups', 'upvote', 'url', 'user_reports', 'visited', 'vote']\n"
     ]
    }
   ],
   "source": [
    "#Now we can inspect the items\n",
    "\n",
    "submission0 = submissions[0]\n",
    "print(dir(submission0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you're familiar with an object's methods and attributes, from that point forward you don't need to force the generator into a list. You can just use the generator with a for loop and process the objects inside the loop. For example, now I've learned all submissions have an attribute ``score`` that tells me what the score of the submission is (the number of upvotes minus downvotes). Now I'll iterate through this user's submissions using a generator and collect all the scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for submission in user.get_submitted(limit=50):\n",
    "    scores.append(submission.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5214, 3503]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instead look at this user's comments and get the score for each comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_comments_with_scores = []\n",
    "\n",
    "for comment in user.get_comments(limit=50):\n",
    "    user_comments_with_scores.append((comment.body, comment.score))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``user_comments_with_scores`` is a list of tuple. In each tuple, the first item is the text of the comment, the second item is the score. \n",
    "\n",
    "Here's the first tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"I think the answer to that is clear and I'll let you fill in the blanks.\",\n",
       " 1918)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_comments_with_scores[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got the text, and we know it's total score, so it's possible to get a bunch of comments and also a numeric value representing how well received thet comment was. \n",
    "\n",
    "# Pulling Content from Subreddits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What really makes Reddit a unique target for Social Media Mining is the concept of *subreddits*. A subreddit, as the name implies, is a section of Reddit that is dedicated to a certain topic or theme. Any user can make any subreddit about almost anything, so long as the topic does not violate Reddit's terms of service. \n",
    "\n",
    "For those of us using data mining algorithms, one of the major advantages of the idea of subreddits is that posts and comments on Reddit are *already* divided into useful categories. For example, there subreddit ``The_Donald`` is a subreddit for supporters of Trump, and the subreddit ``hillaryclinton``is for supporters of Clinton. If you're interested in how supporters of different parties use language differently, you could pull thousands of comments from these subreddits and have a set of data ready for machine learning, with some carrying the label ``trump_supporter`` and the other carrying the label ``clinton_supporter``. \n",
    "\n",
    "Furthermore, the data on Reddit becomes more interesting when examined in conjunction with Reddit's voting system. Any submission or comment can be given a positive vote (an upvote) or a negative vote (a downvote). Presumably, that which gets upvoted at ``The_Donald`` would be quite different from things that get upvoted at ``hillaryclinton``. \n",
    "\n",
    "Let's learn how to pull content from subreddits. At first, I was using the Trump and Clinton subreddits as examples, but that got ugly fast - there was a lot of offensive material. So instead, I'll just use the subreddits for each of the parties. \n",
    "\n",
    "We can pull a subreddit as an object into our Python environment using the method ``get_subreddit``. Remember, we saved our Reddit API access in the variable ``r``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subreddit_republican = r.get_subreddit(\"Republican\")\n",
    "subreddit_democrat = r.get_subreddit(\"democrats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the most recent comments from each of these subreddits, using the function ``get_comments``. Remember, these return generators so we have to iterate through them with a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments_republican = []\n",
    "comments_democrat = []\n",
    "\n",
    "for c in subreddit_republican.get_comments(limit=10):\n",
    "    comments_republican.append((c.body, c.score))\n",
    "    \n",
    "for c in subreddit_democrat.get_comments(limit=10):\n",
    "    comments_democrat.append((c.body, c.score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first comments in the Trump and Clinton subreddits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"I don't follow that logic flow.\\n\\nWat?\", 1)\n",
      "**************************************************\n",
      "(\"No. She has pneumonia. It's not a death sentence. She will recover and be fine. I just wish her campaign didn't hide stuff... AGAIN. I really don't know what this campaign is thinking. It's getting scary.\", 1)\n"
     ]
    }
   ],
   "source": [
    "print(comments_republican[0])\n",
    "print(\"*\"*50)\n",
    "print(comments_democrat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to see what comments are actually popular in each forum? Note that the ``get_comments`` function retrieves the newest comments, so most of the scores for these comments are 1, the default value for a newly created comment. \n",
    "\n",
    "We'll have to think differently about how to get comments that have been around long enough to get a big score (either positive or negative). \n",
    "\n",
    "Reddits are composed of **submissions**, which in turn can have **comments** - people discussion the submission. Both submissions and comments can be voted on. \n",
    "\n",
    "Why don't we get the highest-rated submissions from the last week from each subreddit, and *then* get comments from those submissions? If something is highly rated, it's visible, and a more visible submissions is more likely to generate a more lively discussion. \n",
    "\n",
    "A subreddit object has a method called ``get_top_from_week`` that will return the highest ratest submissions from the last week. As you can imagine, there are also methods like ``get_top_from_hour`` and ``get_top_from_month``.\n",
    "\n",
    "Let's get the highest rated submissions from each subreddit for the last week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_top_submissions = []\n",
    "d_top_submissions = []\n",
    "\n",
    "for s in subreddit_republican.get_top_from_week(limit=10):\n",
    "    r_top_submissions.append((s.title, s.score)) # Getting submission title and score\n",
    "    \n",
    "for s in subreddit_democrat.get_top_from_week(limit=10):\n",
    "    d_top_submissions.append((s.title, s.score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Video: Hillary Clinton having Medical Episode in New York Today', 70)\n",
      "('Obama Just Blasted NBC\\'s Matt Lauer For Letting Trump Lie Through His Teeth | \"I think the most important thing for the public and the press is to just listen what he says and follow up, and ask questions about what appear to be either contradictory or uninformed or outright bad ideas.\"', 158)\n"
     ]
    }
   ],
   "source": [
    "print(r_top_submissions[0])\n",
    "print(d_top_submissions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of getting the submissions, let's get their comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r_comments = []\n",
    "d_comments = []\n",
    "\n",
    "for s in subreddit_republican.get_top_from_week(limit=10):\n",
    "    for c in s.comments:\n",
    "        r_comments.append((c.body, c.score))\n",
    "    \n",
    "for s in subreddit_democrat.get_top_from_week(limit=10):\n",
    "    for c in s.comments:\n",
    "        d_comments.append((c.body, c.score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have comments from the top submissions in each of the subreddits. Now these comments have scores other than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Holy fuck! Look at her leaning against that post, then severely stagger a few steps towards the van being held up by her medical handler, as the Secret Service try to block the view.\\n\\n\\nEDIT #1: The MSM is saying that she \"overheated\" early this morning at a 9/11 function in NYC. FYI, the weather record states it was around 75°F. If she can\\'t handle 75°F, she better stay out of Phoenix, it\\'s 93°F right now at 10:00AM.', 21)\n",
      "**************************************************\n",
      "(\"We're going to do something and it's going to be great is NOT an answer to ANY question.\", 3)\n"
     ]
    }
   ],
   "source": [
    "print(r_comments[0])\n",
    "print(\"*\"*50)\n",
    "print(d_comments[0])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
